{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6a4290c",
   "metadata": {},
   "source": [
    "## Hardware and software hierarchies\n",
    "CUDA is a parallel computing platform and application programming interface ment for general purpose computing on graphics processing unit(GPGPU). CUDA is based on C programming, its compiler is called NVCC.\n",
    "![NVCC](NVCC.png)\n",
    "In CUDA programming host refers to the CPU and its global memory and device refers to the GPU and its global memory. The structure of a GPU hardware is hierarchical, it has 4 main levels, GPU, GPU has many streaming multi processors(SM), each SM is divided into multiple partitions and the partitions are made of cores. So the software also needs a similar level structure for execution, it has the below hierarchical structure.\n",
    "* Grid : Each kernel(GPU function) is executed in a grid. A grid has multiple blocks.\n",
    "* Block/Thread block : The kernel(function) that needs to be executed on the GPU is divided into blocks. A block corresponds to a SM. So the first step for a kernel execution is to configure the number of blocks needed for that kernel i.e it configures the number of SM's needed to execute that kernel. GigaThread unit is responsible for efficiently assigning blocks to SM's.\n",
    "* Warp : Each block is sub divided into warps. Each SM partition will get a fraction of these warps. For example if each block has 32 warps and the SM has 4 partions then each partition will get 8 warps. Warp scheduler unit(present in each partition) is responsible to manage the warps.\n",
    "* Thread : A warp contains 32 threads, each thread will be executed on a seperate core. Number of threads with in a block is another key paramater to be configured for the kernel execution. We can get the number of warps within a block by dividing the number of threads by 32."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118df9b7",
   "metadata": {},
   "source": [
    "## Hello world program\n",
    "Include the cuda and stdio headers. Add a kernel(test01) and a function(main). A kernel is a function that is going to be executed on the GPU, use the __global__ specifier to define a kernel. The normal function will be executed on the CPU. A kernel call is like a function with <<< syntax to give the kernel parameters.\n",
    "```\n",
    "#include \"cuda_runtime.h\"\n",
    "#include \"device_launch_parameters.h\"\n",
    "\n",
    "#include <stdio.h>\n",
    "\n",
    "__global__ void test01()\n",
    "{\n",
    "    //print the blocks and the threads IDs\n",
    "    int warp_id = 0;\n",
    "    warp_id = threadIdx.x/32;  //since each warp has 32 threads\n",
    "    printf(\"\\n The block ID is %d --- The thread ID is %d -- The warp ID is %d\", blockIdx.x, threadIdx.x, warp_id);\n",
    "}\n",
    "\n",
    "int main()\n",
    "{\n",
    "    //kernel execution - kernel_name<<<num_of_blocks, num_of_threads_per_block>>>();\n",
    "    test01 <<<2, 8>>> ();\n",
    "    //Cpu will wait for the GPU to complete the kernel execution\n",
    "    cudaDeviceSynchronize();\n",
    "\n",
    "    return 0;\n",
    "}\n",
    "```\n",
    "blockIdx and threadIdx are predefined variables which contain block ID and thread ID. The CUDA numbers(parameters) can be distributed in different dimensions(x,y and z) based on the requirement. Use nvcc to build the application, on windows nvcc will be integrated to visaul studio community so it can be built directly in IDE. In Linux we can use the nvcc commands to build the code. On successful build we can run the application. The 2 kernel parameters are number of blocks and number of threads per block, these values should be powers of 2. The prints of this program will be random, as any thread on any block can be completed in any order as they are all running in parallel. The number of blocks and number of threads per block cannot exceed the maximum values supported by the GPU, for example if your local GPU supports a max of 1024 threads per block and if you give it a value of 2048 then the kernel execution will fail. We can get the maximum values support by the GPU from its white paper. So in this case if we want to run 2048 threads we have to use a minimum of 2 blocks. cudaDeviceSynchronize() method is used for syncronization between CPU and GPU, CPU will wait at the method for the GPU to complete the execution of the kernel and return."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69119002",
   "metadata": {},
   "source": [
    "## Compiling CUDA on Linux\n",
    "* Command to get the verison of the nvcc installed : nvcc --version\n",
    "* nvcc -o project001 project001.cu : Command to compile a cuda file(project001.cu in this example command). -o option is used to specify the name of the output executable file.\n",
    "* ./project001 : Run the executable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32b7e80",
   "metadata": {},
   "source": [
    "## Vector addition\n",
    "This program adds 2 vectors with 1024 elements each by index. We choose to use 1 block and 1024 threads per block. This choice aligns the number of threads with the number of elements in the vector, thus we can use the thread id's to access the elements in the vector. All addition will be done in parallel on 1024 cores. Below are the essential steps for any CUDA program.\n",
    "* Allocate memory for input and output on the host(CPU) and device(GPU). For this program allocate memory for vectors A,B,C on the host and the decice.\n",
    "* Initialize the inputs on the CPU. In this case initialize vectors A and B on the CPU.\n",
    "* Copy the inputs to the device. Copy the vectors A and B to the GPU, as the addition will be done on the GPU.\n",
    "* Once the inputs are on the device, define the kernel to be executed. In this case a 'vectorAdd' kernel which adds the device vectors A and B elementwise and saves the results to device vector C.\n",
    "* Launch the kernel with suitable number of blocks and threads per block giving it the device parameters(kernel function parameters). For this program 1 block and 1024 threads per block.\n",
    "* The kernel stores the results in the GPU memory, so this next step is to copy the results back to the CPU. Copy device vector C to host vector C. This result can then be used for furthur computation as per the program requirement.\n",
    "* The final step is to free the allocated memory on both the host and the device.\n",
    "\n",
    "```\n",
    "#include \"cuda_runtime.h\"\n",
    "#include \"device_launch_parameters.h\"\n",
    "#include <memory>\n",
    "#include <stdio.h>\n",
    "\n",
    "#define SIZE 1024  //Define the size of the vectors\n",
    "\n",
    "//kernel for vector addition\n",
    "__global__ void vectorAdd(int *A, int *B, int *C, int n)\n",
    "{\n",
    "    int i = threadIdx.x;\n",
    "    C[i] = A[i] + B[i];\n",
    "}\n",
    "\n",
    "int main()\n",
    "{\n",
    "    int *A, *B, *C;  //host vectors\n",
    "    int *d_A, *d_B, *d_C;  //device vectors\n",
    "    int size = SIZE * sizeof(int);\n",
    "\n",
    "    //allocate host vectors on the CPU memory\n",
    "    A = (int*)malloc(size);\n",
    "    B = (int*)malloc(size);\n",
    "    C = (int*)malloc(size);\n",
    "\n",
    "    //allocate device vectors on the GPU memory\n",
    "    cudaMalloc((void**)&d_A, size);\n",
    "    cudaMalloc((void**)&d_B, size);\n",
    "    cudaMalloc((void**)&d_C, size);\n",
    "\n",
    "    //initialize the host vectors\n",
    "    for (int i = 0; i < SIZE; i++)\n",
    "    {\n",
    "        A[i] = i;\n",
    "        B[i] = SIZE - i;\n",
    "    }\n",
    "\n",
    "    //copy host vectors to device\n",
    "    cudaMemcpy(d_A, A, size, cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(d_B, B, size, cudaMemcpyHostToDevice);\n",
    "\n",
    "    //Execute the kernel\n",
    "    vectorAdd << <1, 1024 >> > (d_A, d_B, d_C, SIZE);\n",
    "    \n",
    "    //copy result back to the host\n",
    "    cudaMemcpy(C, d_C, size, cudaMemcpyDeviceToHost);\n",
    "\n",
    "    printf(\"\\nExecution finished\\n\");\n",
    "    for (int i = 0; i < SIZE; i++)\n",
    "    {\n",
    "        printf(\"%d + %d = %d\", A[i], B[i], C[i]);\n",
    "        printf(\"\\n\");\n",
    "    }\n",
    "\n",
    "    //cleanup\n",
    "    cudaFree(d_A);\n",
    "    cudaFree(d_B);\n",
    "    cudaFree(d_C);\n",
    "    free(A);\n",
    "    free(B);\n",
    "    free(C);\n",
    "\n",
    "    return 0;\n",
    "}\n",
    "```\n",
    "\n",
    "Now say that we want to add vectors of sizes 2048 and maximum threads supported by a block is 1024 for your GPU. To solve this we have to use more than 1 block, the number of vector elements must match the total number of threads run by the kernel. The number of blocks * number of threads per block, will give the total threads that will be run by the kernel. For this case we can use 2 blocks with each having 1024 threads. This will result in the vector index calculation in the kernel to use the blockid along with the threadid, int i = threadIdx.x + blockIdx.x * blockDim.x(number_of_threads_per_block). Use the predefined variable blockDim to get the number of threads per block in the x direction. Below is the updated kernel and its call.\n",
    "```\n",
    "//kernel for vector addition\n",
    "__global__ void vectorAdd(int* A, int* B, int* C, int n)\n",
    "{\n",
    "    int i = threadIdx.x + blockIdx.x * blockDim.x;\n",
    "    C[i] = A[i] + B[i];\n",
    "}\n",
    "\n",
    "//Execute the kernel\n",
    "vectorAdd << <2, 1024 >> > (d_A, d_B, d_C, SIZE);\n",
    "```\n",
    "How many SM's will be used to execute this kernel and how will it impact the performance. Since we are using 2 blocks only 2 SM's will be used for the execution of this kernel. If the total number of the SM's on the GPU are 108 then our GPU utilization is less than 2%. This GPU utilization is a key indicator of the GPU performance. In order to increase the SM's we can increase the block count while decreasing the number of threads per block. The minimum number of threads that can be used in a block is 32(1 warp), which will result 64 blocks being used, thus using 64 SM's and increasing the GPU utilization.\n",
    "We can validate the performance improvement by measuring the execution times in both the cases. We can calculate the execution times using 2 ways, one way is to use CUDA's API's and the other is to use the profilers(Nsight systems and Nsight compute), here we will check with the CUDA API method.  \n",
    "We can create CUDA events and record as needed to measure time in code as shown below.\n",
    "```\n",
    "#include <cuda.h>\n",
    "#include \"cuda_runtime.h\"\n",
    "#include \"device_launch_parameters.h\"\n",
    "#include <memory>\n",
    "#include <stdio.h>\n",
    "\n",
    "#define SIZE 2048  //Define the size of the vectors\n",
    "\n",
    "//kernel for vector addition\n",
    "__global__ void vectorAdd(int* A, int* B, int* C, int n)\n",
    "{\n",
    "    int i = threadIdx.x + blockIdx.x * blockDim.x;\n",
    "    C[i] = A[i] + B[i];\n",
    "}\n",
    "\n",
    "int main()\n",
    "{\n",
    "    int* A, * B, * C;  //host vectors\n",
    "    int* d_A, * d_B, * d_C;  //device vectors\n",
    "    int size = SIZE * sizeof(int);\n",
    "\n",
    "    //allocate host vectors on the CPU memory\n",
    "    A = (int*)malloc(size);\n",
    "    B = (int*)malloc(size);\n",
    "    C = (int*)malloc(size);\n",
    "\n",
    "    //allocate device vectors on the GPU memory\n",
    "    cudaMalloc((void**)&d_A, size);\n",
    "    cudaMalloc((void**)&d_B, size);\n",
    "    cudaMalloc((void**)&d_C, size);\n",
    "\n",
    "    //initialize the host vectors\n",
    "    for (int i = 0; i < SIZE; i++)\n",
    "    {\n",
    "        A[i] = i;\n",
    "        B[i] = SIZE - i;\n",
    "    }\n",
    "\n",
    "    //copy host vectors to device\n",
    "    cudaMemcpy(d_A, A, size, cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(d_B, B, size, cudaMemcpyHostToDevice);\n",
    "\n",
    "    //create events for timing\n",
    "    cudaEvent_t start, stop;\n",
    "    cudaEventCreate(&start);\n",
    "    cudaEventCreate(&stop);\n",
    "\n",
    "    //Execute the kernel and time\n",
    "    cudaEventRecord(start);\n",
    "    vectorAdd << <64, 32 >> > (d_A, d_B, d_C, SIZE);\n",
    "    cudaEventRecord(stop);\n",
    "\n",
    "    //copy result back to the host\n",
    "    cudaMemcpy(C, d_C, size, cudaMemcpyDeviceToHost);\n",
    "\n",
    "    cudaEventSynchronize(stop);\n",
    "    float milliseconds = 0;\n",
    "    cudaEventElapsedTime(&milliseconds, start, stop);\n",
    "    printf(\"Execution time : %f milliseconds\\n\", milliseconds);\n",
    "\n",
    "    //cleanup\n",
    "    cudaFree(d_A);\n",
    "    cudaFree(d_B);\n",
    "    cudaFree(d_C);\n",
    "    free(A);\n",
    "    free(B);\n",
    "    free(C);\n",
    "\n",
    "    return 0;\n",
    "}\n",
    "```\n",
    "Change the number of blocks and the threads per block and measure the timings, we will get the best time when we use 64 blocks and 32 threads per block. The better executuion times is due to the samller block size that can be better managed by a single SM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec9d6e6",
   "metadata": {},
   "source": [
    "## Extra large vector addition\n",
    "Here we are going to look at adding two vectors with more than 400 million elements each. We are going to use 1024 threads per block(which is the maximum threads per block) and 1024*432 blocks to support this addition. Below is the code.\n",
    "```\n",
    "#include <cuda.h>\n",
    "#include \"cuda_runtime.h\"\n",
    "#include \"device_launch_parameters.h\"\n",
    "#include <memory>\n",
    "#include <stdio.h>\n",
    "\n",
    "#define SIZE 1024*432*1024  //Define the size of the vectors, more tha 400 million elements\n",
    "\n",
    "//kernel for vector addition\n",
    "__global__ void vectorAdd(int* A, int* B, int* C, int n)\n",
    "{\n",
    "    int i = threadIdx.x + blockIdx.x * blockDim.x;\n",
    "    C[i] = A[i] + B[i];\n",
    "}\n",
    "\n",
    "void random_ints(int *x, int size)\n",
    "{\n",
    "    for (int i = 0; i < size; i++)\n",
    "    {\n",
    "        x[i] = rand() % 100;\n",
    "    }\n",
    "}\n",
    "\n",
    "int main()\n",
    "{\n",
    "    int* A, * B, * C;  //host vectors\n",
    "    int* d_A, * d_B, * d_C;  //device vectors\n",
    "    int size = SIZE * sizeof(int);\n",
    "\n",
    "    //allocate host vectors on the CPU memory\n",
    "    A = (int*)malloc(size);\n",
    "    B = (int*)malloc(size);\n",
    "    C = (int*)malloc(size);\n",
    "\n",
    "    //allocate device vectors on the GPU memory\n",
    "    cudaMalloc((void**)&d_A, size);\n",
    "    cudaMalloc((void**)&d_B, size);\n",
    "    cudaMalloc((void**)&d_C, size);\n",
    "\n",
    "    //initialize the host vectors\n",
    "    random_ints(A, SIZE);\n",
    "    random_ints(B, SIZE);\n",
    "\n",
    "    //copy host vectors to device\n",
    "    cudaMemcpy(d_A, A, size, cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(d_B, B, size, cudaMemcpyHostToDevice);\n",
    "\n",
    "    //Execute the kernel\n",
    "    //num_of_blocks * num_of_threads_pre_block should be equal to the number of elements in each vector\n",
    "    vectorAdd << <1024*432, 1024 >> > (d_A, d_B, d_C, SIZE);\n",
    "    \n",
    "    cudaDeviceSynchronize();\n",
    "\n",
    "    //copy result back to the host\n",
    "    cudaMemcpy(C, d_C, size, cudaMemcpyDeviceToHost);\n",
    "\n",
    "    printf(\"\\nFinished\\n\");\n",
    "\n",
    "    //print the first 100 elements\n",
    "    for (int i = 0; i < 100; i++)\n",
    "    {\n",
    "        printf(\"\\nElement ID = %d ---> %d + %d = %d\", i, A[i], B[i], C[i]);\n",
    "    }\n",
    "\n",
    "    //cleanup\n",
    "    cudaFree(d_A);\n",
    "    cudaFree(d_B);\n",
    "    cudaFree(d_C);\n",
    "    free(A);\n",
    "    free(B);\n",
    "    free(C);\n",
    "\n",
    "    return 0;\n",
    "}\n",
    "```\n",
    "This will work as long as we have enough RAM on the CPU. Now say we increased the vector size to 1024*1024*1204(over a billion elements), this might result in not enough memeory(RAM) on the CPU as 3 vectors of this size are being created, and each vector is of size 4*1024*1024*1024 which is 4GB. Similarly we have to lookout for the memory on the GPU too. The solution is to divide the vectors into chunks and process a chunk at a time.\n",
    "```\n",
    "#include <cuda.h>\n",
    "#include \"cuda_runtime.h\"\n",
    "#include \"device_launch_parameters.h\"\n",
    "#include <memory>\n",
    "#include <stdio.h>\n",
    "\n",
    "#define TOTAL_SIZE 1024*1024*1024  //Define the size of the vectors, more than 1 billion elements\n",
    "#define CHUNK_SIZE 1024*1024*128   //Each chunk size, total vector divided into 8 pieces\n",
    "#define BLOCK_SIZE 1024            //Number of threads per block\n",
    "\n",
    "//kernel for vector addition\n",
    "__global__ void vectorAdd(int* a, int* b, int* c, int chunkSize)\n",
    "{\n",
    "    int i = threadIdx.x + blockIdx.x * blockDim.x;\n",
    "    if (i < chunkSize) //Ensure we do not go out of bounds\n",
    "    {\n",
    "        c[i] = a[i] + b[i];\n",
    "    }\n",
    "}\n",
    "\n",
    "void random_ints(int *x, int size)\n",
    "{\n",
    "    for (int i = 0; i < size; i++)\n",
    "    {\n",
    "        x[i] = rand() % 100;\n",
    "    }\n",
    "}\n",
    "\n",
    "int main()\n",
    "{\n",
    "    int* chunk_a, * chunk_b, * chunk_c;  //host vectors\n",
    "    int* d_a, * d_b, * d_c;  //device vectors\n",
    "    int chunkSizeInBytes = CHUNK_SIZE * sizeof(int);\n",
    "\n",
    "    //allocate host vectors on the CPU memory\n",
    "    chunk_a = (int*)malloc(chunkSizeInBytes);\n",
    "    chunk_b = (int*)malloc(chunkSizeInBytes);\n",
    "    chunk_c = (int*)malloc(chunkSizeInBytes);\n",
    "\n",
    "    //allocate device vectors on the GPU memory\n",
    "    cudaMalloc((void**)&d_a, chunkSizeInBytes);\n",
    "    cudaMalloc((void**)&d_b, chunkSizeInBytes);\n",
    "    cudaMalloc((void**)&d_c, chunkSizeInBytes);\n",
    "\n",
    "    //calculate the number of blocks needed for the kernel to execute\n",
    "    int numBlocks = (CHUNK_SIZE + BLOCK_SIZE - 1) / BLOCK_SIZE;\n",
    "\n",
    "    //loop through chunk by chunk and execute the kernel\n",
    "    for (long long offset = 0; offset < TOTAL_SIZE; offset += CHUNK_SIZE)\n",
    "    {\n",
    "        int currentChunkSize = (TOTAL_SIZE - offset) < CHUNK_SIZE ? TOTAL_SIZE - offset : CHUNK_SIZE;\n",
    "        printf(\"\\nOffset %d\\n\", offset);\n",
    "\n",
    "        //populate the chunks with random data\n",
    "        random_ints(chunk_a, CHUNK_SIZE);\n",
    "        random_ints(chunk_b, CHUNK_SIZE);\n",
    "\n",
    "        //copy chunks to the GPU\n",
    "        cudaMemcpy(d_a, chunk_a, chunkSizeInBytes, cudaMemcpyHostToDevice);\n",
    "        cudaMemcpy(d_b, chunk_b, chunkSizeInBytes, cudaMemcpyHostToDevice);\n",
    "\n",
    "        //launch the kernel\n",
    "        vectorAdd << <numBlocks, BLOCK_SIZE >> > (d_a, d_b, d_c, CHUNK_SIZE);\n",
    "\n",
    "        //copy result back to the host chunk\n",
    "        cudaMemcpy(chunk_c, d_c, chunkSizeInBytes, cudaMemcpyDeviceToHost);\n",
    "\n",
    "        //In real applications chunk_c will be used for furthur processing\n",
    "        //print the first 5 elements of every chunk\n",
    "        for (int i = 0; i < 5; i++)\n",
    "        {\n",
    "            printf(\"\\nElement ID = %d ---> %d + %d = %d\", i, chunk_a[i], chunk_b[i], chunk_c[i]);\n",
    "        }\n",
    "    }\n",
    "\n",
    "    printf(\"\\nFinished\\n\");\n",
    "\n",
    "    //cleanup\n",
    "    cudaFree(d_a);\n",
    "    cudaFree(d_b);\n",
    "    cudaFree(d_c);\n",
    "    free(chunk_a);\n",
    "    free(chunk_b);\n",
    "    free(chunk_c);\n",
    "\n",
    "    return 0;\n",
    "}\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
