{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dcc41743",
   "metadata": {},
   "source": [
    "## GPU vs CPU\n",
    "![CPUvsGPU](CPUvsGPU.png)  \n",
    "\n",
    "Both CPU and GPU have similar components, both have DRAM or main memory, both have different cache memory levels(orange), both have ALU and control units(grey). While the CPU has a few powerful ALU's, the GPU has thousands of small ALU's or cores. CPU ALU's are general purpose and optimized to run a single instruction on a thread, GPU ALU's are specialized and optimized to run the same instuction on multiple ALU's in parallel.  \n",
    "In cases where we have dependent instructions(current instruction is dependent on the output of the previous instruction), we have to execute the instructions in sequence, this kind of execution is ideal for CPU's. In cases where we have independent instructions(each instruction is independent of the other instructions), we can execute these instructions simultaneously using seperate cores, this kind of execution is ideal for GPU's.  \n",
    "On a mother board the GPU, network card and sound card are connected to the CPU on a PCI, so we can plugin a GPU into the PCI slot on the mother board.  \n",
    "![CPUGPUConnection](CPUGPUConnection.png)  \n",
    "The CPU's main memory(DRAM in motherboard's memory slot) is connected to the CPU throught memeory controller on a bus. For the GPU the main memeory is inside the GPU card.  \n",
    "Now let us look at the key components of a GPU.\n",
    "![GPUComponents](GPUComponents.png)  \n",
    "The GPU is mainly composed of streaming multiprocessors(SM), various memory levels such as L2 cache memory and the global memory, and elements like the scheduler and the dispatcher are also present. L2 cache memory is accessible to all the SM's, the global memory is connected to the SM's through the memory controllers. It contains NVlink units ment for communication with the CPU and other GPU's, there is also a PCI interface ment for slower communications.   \n",
    "Diving into a single SM, it will contain many cores, L1 cache memory, shared memory, registers, its own schedular and dispatcher. The cores are designed for specific tasks, like some cores are for floating point operations, some are for integer operations, some others are for tensor operations(matrix operations) and some for special functions(like log calculations). \n",
    "![SMComponents](SMComponents.png)  \n",
    "The L1 cache memory is inside a SM, L2 cache memory is on the GPU. The SM also contains load and store units, these units are responsible for reading/writing data from memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5224a6e",
   "metadata": {},
   "source": [
    "## Nvidia's GPU's categories and architectures\n",
    "Architecture refers to the underlying design and structure of the GPU, it determines efficiency, performance and capabilities. Nvidia has 2 categories(generations), standard and HPC(high performance computing) GPU's. Standard are ment for normal users with generations like Tegra(for mobile devices), GeForce(for PC and laptop, gamers and graphics) and Quadro/RTX(professional work station cards for engineers and designers). HPC GPU's are ment for HPC companies with generations like Tesla. While generations specify who can use those GPU's where as the architecture refers to the GPU's design and capability, example Nvidia architectures are Hopper, Amphere and Volta. Thus we can say that each architecture may contain various generations, for example both RTX 3090(GeForce) and A100(Tesla) are based on Amphere architecture. A GPU name will contain both generation and the architecutre, for example in the 'RTX A6000' GPU, RTX refers to the generation and A refers to the Amphere architecture.  \n",
    "For comparing various GPU's and GPU's from various companies we can use the TechPowerUp website. The Tesla generations do not have a fan on them as these are ment for data centers which already have large cooling systems.  \n",
    "The GPU chip is the actual silicon cores, memmory controllers and other circuitry, it does not include global memory and cooling/peripherals. Where as the GPU is the final product, it includes GPU chip, global memory and peripheral interfaces(display ports, cooling fan etc). In the specification of the GPU, the GPU chip will also be specified.  \n",
    "The main parameters to assess the performance and capabilities of a GPU are \n",
    "* Memory bandwidth : The memory bandwidth is dependent memory speed and bus width. Memory speed(also called memory clock speed) is how much data can the memory process per second, typically in GB/s. Increasing the bus width increases the amount of data that can be transfered at a given time. Memory speed also depends on the memory technology(DDR, HBM etc).  \n",
    "* Throughput TFLOPS(instructions/second) : It is dependent on the core count and core speed. More cores will execute more instructions simultaneously(for tasks that can be parallelized) thus resulting in higher throughput. The core speed which is the clock speed is equally important parameter to determine the GPU speed. Higher cores with higher clock speeds will result in inceased power consumption.  \n",
    "* Features(core types) : We have to consider all the core types and novel units(like tensor cores) in the GPU.  \n",
    "The speed of the operations also depends on the data types and thier sizes like the CPU, for example floating point math is slower than interger math.  \n",
    "Nvidia GPU's have compute capability version number(CC), accross its architectures. From the CC number we can identify the features supported by that GPU. In the CUDA programming documentation has tables to show features supported based on CC number. The CC version also determines the compatible CUDA version for that GPU, which inturn support the corresponding features. We cannot use a lower CUDA version than specified for the CC number."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf15ea1",
   "metadata": {},
   "source": [
    "## White papers\n",
    "White papers are the data sheets/technical specifications of the GPU's and their architecture. Type GPU chip name followed by white paper to find the chips white paper on web. White paper will also have details of how it compares itself to other architectures. All white paper's will have sections for introduced new features; streaming multiprocessor internal structure, architecture and components; performance bench mark comparisions; and technical specifications like core count, memmory details, power requirements etc.  \n",
    "Typically a single SM is divided into multiple partitions(each partition containing same units) for better performance as shown below.  \n",
    "![SMPartitions](SMPartitions.png)  \n",
    "Each partition will typically have the below units.\n",
    "* L0 instruction cache : Stores the instructions for the cores to execute, for faster access of the instructions. This is an instruction cache unlike L1 which is a data cache.\n",
    "* Warp scheduler : Is responsible for managing warps. Each GPU application runs on multiple grids on the GPU, each grid is divided into multiple blocks running on SM's. Each block can contain many threads, each group of 32 threads within a block is called a warp. The 32 threads in a warp will always execute the same instruction in parallel on different cores(SIMD, also called SIMT(single instruction multiple threads)). The warp scheduler will determine the strategy of how different warps will be scheduled for execution, while the next unit, dispatcher executes that strategy.\n",
    "* Dispatch unit : Executes the warps.\n",
    "* Register file : Contains a large number of registers(fast memory locations), to hold the temporary data needed during execution.\n",
    "* Computational units : Different cores.\n",
    "All the partitions in a SM share the same L1 cache memeory, shared memeory and texure memory."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
