{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad4eb4bf",
   "metadata": {},
   "source": [
    "A data structure has multiple elements, multiple threads may access these elements, these accesses can conflit, locks or atomic operations may be needed. Modifying one part of the data structure object may effect other parts of the object, with linked lists adding/removing elements modifies the surrounding nodes, with vector/string/dynamic array adding/removing elements in the middle will move the following elements in the memory and adding elements may cause the block to be reallocated, if during these times other threads are accesses those elements, pointers and references may dangle, iterators may become invalidated.  \n",
    "STL containers are \"memory objects\", concurrent reads from different threads of the same container object are safe, single write is also safe, concurrent reads and writes are not safe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305d3a79",
   "metadata": {},
   "source": [
    "## std::shared_ptr\n",
    "Different instances share the same memeory location, it uses reference counting. When a shared_ptr object is copied/assigned there are no memory operations, only the reference counter is incremented, when a copy is destroyed the reference counter is decremented, when the last copy is destroyed the reference counter becomes zero thus allocated memory is released.\n",
    "```\n",
    "//Pass a pointer to the constructor\n",
    "std::shared_ptr<int> ptr1(new int(42));\n",
    "\n",
    "//Calling the make_shared is better, data and reference counter close in memory, faster access\n",
    "std::shared_ptr<int> ptr2 = std::make_shared<int>(42);\n",
    "```\n",
    "std::unique_ptr has the same overhead as a traditional pointer, std::shared_ptr has more overhead, use it only when necessary.\n",
    "When using std::shared_ptr in threads context there are potential issues of conflicting access of its data and reference counter. The reference counter is a atomic type in std::shared_ptr, hence it is safe to copy/move/assign it multi threaded programs, however the syncronization of the data that the shared_ptr is using is the programmers responsibility, we can also use std::atomic\\<std::shared_ptr\\>.\n",
    "```\n",
    "std::shared_ptr<int> shptr = std::make_shared<int>(42);\n",
    "\n",
    "//Mutex to protect std::shared_ptr's data\n",
    "std::mutex mut;\n",
    "\n",
    "void func1()\n",
    "{\n",
    "    std::lock_guard<std::mutex> lgd(mut);\n",
    "    *shptr = 5;\n",
    "}\n",
    "\n",
    "void func2()\n",
    "{\n",
    "    std::lock_guard<std::mutex> lgd(mut);\n",
    "    *shptr = 7;\n",
    "}\n",
    "\n",
    "int main()\n",
    "{\n",
    "    std::thread thr1(func1);\n",
    "    std::thread thr2(func2);\n",
    "    \n",
    "    thr1.join();\n",
    "    thr2.join();\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5263503a",
   "metadata": {},
   "source": [
    "## Monitor Class\n",
    "A monitor class is a class that is internally synchronized, users of the class don't need to wary about synchronization.\n",
    "```\n",
    "class bank\n",
    "{\n",
    "\n",
    "public:\n",
    "    void debit(const std::string& name, int amount)\n",
    "    {\n",
    "        std::lock_guard<std::mutex> lck(mut);\n",
    "        //Read/write shared data\n",
    "    }\n",
    "    \n",
    "    void credit(const std::string& name, int amount)\n",
    "    {\n",
    "        std::lock_guard<std::mutex> lck(mut);\n",
    "        //Read/write shared data\n",
    "    }\n",
    "    \n",
    "    void print(const std::string& name)\n",
    "    {\n",
    "        std::lock_guard<std::mutex> lck(mut);\n",
    "        //Read/write shared data\n",
    "    }\n",
    "\n",
    "private:\n",
    "    std::mutex mut;\n",
    "    //Shared data\n",
    "};\n",
    "```\n",
    "This naive solution has a number of problems. Memeber functions may call other member functions, result in recursive locks which may result in deadlocks. Clients using this class may invoke the member functions multiple times, resulting in many locking and unlocking calls, thus slowing down the application. This solution cannot be applied to classes that cannot be modified.\n",
    "A slightly better solution is to add a wrapper around the object, wrappers member functions lock a mutex and forward the call to the actual object. This solution can be extended to any class but the same other problems exist here too. \n",
    "```\n",
    "class bank_monitor\n",
    "{\n",
    "\n",
    "public:\n",
    "    void debit(const std::string& name, int amount)\n",
    "    {\n",
    "        std::lock_guard<std::mutex> lck(mut);\n",
    "        bank.debit(name, amount);\n",
    "    }\n",
    "    \n",
    "    void credit(const std::string& name, int amount)\n",
    "    {\n",
    "        std::lock_guard<std::mutex> lck(mut);\n",
    "        bank.credit(name, amount):\n",
    "    }\n",
    "    \n",
    "    void print(const std::string& name)\n",
    "    {\n",
    "        std::lock_guard<std::mutex> lck(mut);\n",
    "        bank.print(name);\n",
    "    }\n",
    "\n",
    "private:\n",
    "    std::mutex mut;\n",
    "    Bank bank; //Bank class will no longer be synchronized internally\n",
    "};\n",
    "```\n",
    "In a spphisticated monitor class, we make it generic, wrapped class type as template parameter, so we can monitor any type of object. It will functor class with overloaded operator() having a callable object as its argument, this callable object should contain a sequence of member function calls that needs to be synchronized(transaction), we lock the mutex and invoke the callable object. This overloaded operator() will also be a template function, callable object as the template parameter.\n",
    "```\n",
    "template <typename T>\n",
    "class monitor\n",
    "{\n",
    "public:\n",
    "    //Constructor which takes the object of the type to monitor\n",
    "    //If object is not given it uses default constructor to create object of the given type\n",
    "    monitor(T data = T{}) : m_data(data)\n",
    "    {\n",
    "    }\n",
    "    \n",
    "    //Argument is a callable object of type F, which takes a argument of type T\n",
    "    template <typename F>\n",
    "    auto operator()(F func)\n",
    "    {\n",
    "        std::lock_guard<mutex> lck(mut);\n",
    "        return func(m_data);\n",
    "    }\n",
    "\n",
    "private:\n",
    "    //The object to be monitored\n",
    "    T m_data;\n",
    "    std::mutex mut;\n",
    "}\n",
    "\n",
    "int main()\n",
    "{\n",
    "    //Monitor wrapper for the bank class\n",
    "    monitor<bank> mon;\n",
    "    \n",
    "    //Invoke monitors function call operator\n",
    "    //pass a callable object which takes a bank argument\n",
    "    mon([](bank b)\n",
    "          {\n",
    "              //Call the member functions, all run under the same lock\n",
    "              b.debit(\"Peter\", 1000);\n",
    "              b.credit(\"Paul\", 1000);\n",
    "              b.print(\"Peter\");\n",
    "              b.print(\"Paul\");\n",
    "          });\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bb9d0f",
   "metadata": {},
   "source": [
    "## Semaphore\n",
    "Semaphore has a counter, acquire() decrements the counter, release() increments the counter, counter can be zero, aquire() will block until the counter becomes positive again. Below is an implementation of it with condition variable.\n",
    "```\n",
    "class semaphore\n",
    "{\n",
    "public:\n",
    "    void release()\n",
    "    {\n",
    "        std::lock_guard<std::mutex> lock(m_mtx);\n",
    "        if(m_counter < MAX_COUNTER)\n",
    "        {\n",
    "            ++m_counter;\n",
    "        }    \n",
    "        m_cv.notify_all();\n",
    "    }\n",
    "    void acquire()\n",
    "    {\n",
    "        std::unique_lock<std::mutex> lock(m_mtx);\n",
    "        while(m_counter == 0)\n",
    "        {\n",
    "            m_cv.wait(lock);\n",
    "        }\n",
    "        --m_counter;\n",
    "    }\n",
    "\n",
    "private:\n",
    "    std::mutex m_mtx;\n",
    "    std::condition_variable m_cv;\n",
    "    int m_counter{0};\n",
    "    int MAX_COUNTER{10}; //If MAX_COUNTER is 1 then it is a binary semaphore\n",
    "};\n",
    "```\n",
    "C++ 20 has now implemented the semaphore, probably implemented by using the OS semaphores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbce929f",
   "metadata": {},
   "source": [
    "## Concurrent Queue\n",
    "Queue is FIFO data structure, new items are added to the back of the queue while items are removed from the front of the queue. The std::queue present in the C++ libarry is not thread safe. Here we are going to implement a concurrent queue with std::queue as the class member.\n",
    "```\n",
    "template <typename T>\n",
    "class concurrent_queue\n",
    "{\n",
    "public:\n",
    "    concurrent_queue() = default;\n",
    "    concurrent_queue(int max) : m_max{max}\n",
    "    {\n",
    "    }\n",
    "    \n",
    "    concurrent_queue(const concurrent_queue&) = delete;\n",
    "    concurrent_queue& operator=(const concurrent_queue&) = delete; \n",
    "    concurrent_queue(concurrent_queue&&) = delete;\n",
    "    concurrent_queue& operator=(const concurrent_queue&&) = delete;\n",
    "    \n",
    "    void push(T value)\n",
    "    {\n",
    "        std::lock_guard<std::mutex> lck(m_mut);\n",
    "        m_queue.push(value);\n",
    "        \n",
    "        while(m_queue.size() > m_max)\n",
    "        {\n",
    "            lck.unlock();\n",
    "            std::this_thread::sleep_for(50ms);\n",
    "            lck.lock();\n",
    "        }\n",
    "        \n",
    "        m_cv.notify_one();\n",
    "    }\n",
    "    \n",
    "    void pop(T& value)\n",
    "    {\n",
    "        std::lock_guard<std::mutex> lck(m_mut);\n",
    "        \n",
    "        //If empty, wait till some data is pushed in\n",
    "        m_cv.wait(lck, [this](){return !m_queue.empty();});\n",
    "        \n",
    "        value = m_queue.front();\n",
    "        m_queue.pop();\n",
    "    }\n",
    "\n",
    "private:\n",
    "    std::queue<T> m_queue;\n",
    "    std::mutex m_mut;\n",
    "    std::condition_variable m_cv;\n",
    "    int m_max{50};\n",
    "};\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102dac43",
   "metadata": {},
   "source": [
    "## Thread Pools\n",
    "Creating a thread requires a lot of work by the OS, like creating a execution stack for the thread, creating internal data to manage the thread and scheduler context switching and executing the thread. Creating a new thread can take 10,000 times as long as calling a function directly. Thus to aviod the overhead of creating threads we use thread pools so that we can reuse threads from the pool as needed. To implement in C++ we can use a container of thread objects, with size matching the number of cores on the machine - 2(to allow for the main thread and the OS). To get the number of cores on the machine use std::thread::hardware_concurrency(). It will have a queue of tasks for the pool of threads to execute, tasks are callable objects. Thread pools work best when you have short simple tasks.\n",
    "```\n",
    "class thread_pool\n",
    "{\n",
    "public:\n",
    "\n",
    "    thread_pool()\n",
    "    {\n",
    "        m_thread_count = std::thread::hardware_concurrency() - 2;\n",
    "        for(int i = 0; i < m_thread_count; i++)\n",
    "        {\n",
    "            threads.push_back(std::thread(thread_pool::worker, this));\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    ~thread_pool()\n",
    "    {\n",
    "        for(std::thread& thr : m_threads)\n",
    "        {\n",
    "            thr.join();\n",
    "        }\n",
    "    }\n",
    "\n",
    "    //User of the thread_pool class can submit new tasks to execute\n",
    "    submit(std::function<void()> func)\n",
    "    {\n",
    "        m_work_queue.push(func);\n",
    "    }\n",
    "\n",
    "private:\n",
    "\n",
    "    void worker()\n",
    "    {\n",
    "        while(true)\n",
    "        {\n",
    "            std::function<void()> task;\n",
    "            //Pop will wait if there are no new tasks to run\n",
    "            m_work_queue.pop(task);\n",
    "            task();\n",
    "        }\n",
    "    }\n",
    "\n",
    "    concurrent_queue<std::function<void()>> m_work_queue;\n",
    "    std::vector<std::thread> m_threads;\n",
    "    int m_thread_count;\n",
    "};\n",
    "```\n",
    "In this example the queue can become bottle neck, as only thread can pop a task for execution at a time. An alternative is to have a seperate queue for each thread, this way a thread never needs to wait to get its next task. To implement this we can use vector of queues above instead of a single queue, in submit we can use round-robin scheduling to put a new task on the next threads queue.  \n",
    "One disadvantage with this one queue per thread approach is that if a thread's queue is empty then that thread is idle even though another queue is have tasks pending in it to execute, this another queue might be running a long running task. One way to solve this problem is to remove round-robin scheduling in submit and push the task into the queue having the least number of elements. Another solution is to employ work stealing, if a thread's queue is empty then the thread steals a task from another thread's queue."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
