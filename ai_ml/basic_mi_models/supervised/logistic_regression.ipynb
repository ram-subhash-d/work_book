{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "944c2c6c-0422-4043-bde7-cefdc2adb57c",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "Performs well for binary clasification, can be extended for multi-class classification. Most common approach is to use a linear decision boundaries, but can be extended to use non-linear boundaries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0a1b4e-8276-415b-80c8-60086375012f",
   "metadata": {},
   "source": [
    "## Model\n",
    "Use a linear boundary to separate the data points into 2 halfs. For a n dimensional data we have to draw a n-1 dimensional hyper plane to split the data points into 2 halfs. So have to find the best hyper plane such that the samples of one class are on one side and the other class are on the other side.\n",
    "\n",
    "Equation of a hyper plane : h(x) = $n^Tx + b = 0$ (n - direction of normal, x - input, b - constant).\\\n",
    "For 2D case $x = \\begin{bmatrix}x_{1}\\\\x_{2}\\end{bmatrix}$, $n^T = \\begin{bmatrix}n_{1} & n_{2}\\end{bmatrix}$, hence the equation will be $\\begin{bmatrix}x_{1}\\\\x_{2}\\end{bmatrix}$$\\begin{bmatrix}n_{1} & n_{2}\\end{bmatrix}$ + b = 0 => $n_{1}x_{1} + n_{2}x_{2} + b = 0$.\\\n",
    "For 3D case $n_{1}x_{1} + n_{2}x_{2} + n_{3}x_{3} + b = 0$ , and so on for higher dimensions\n",
    "\n",
    "For a given data point if h(x) > 0 then sample belongs to one class, if h(x) < 0 then it belongs to the other class. It is just not enough to say that a sample belongs to a class, it will be better to associate a probability based on the confidence that it belongs to a particular class. If the sample is close to the boundary the probabilities will be close to 0.5, if the sample is far off from the boundary then the probability will be close to 1 for one class and 0 for the other class. We will be predicting the probability first and then the class based on the probability values. Since probability is a continous value it becomes a regression problem, hence the name Logistic Regression.\n",
    "\n",
    "In order to model the probabilities we have to transform the decision function h(x) to a value between 0 to 1. For the transformation the function p(x) = $e^{h(x)}/(1 + e^{h(x)})$ is used and is called sigmoid function.\n",
    "\n",
    "So the model is to find the optimal n(vector) and b(scalar) for the equation h(x). Since the original data does not contain probalities, we have to find a way to estimate n and b based on maximizing p(x) or 1 - p(x) depending on the class the sample belongs to, this can be done using Maximum Likelihood Estimation(MLE). Say y=0 for samples that belog to class 1 and y=1 for samples that belong to class 2, the for y=0 1-p(x) must be maximized and for y=1 p(x) must be maximized.\n",
    "\n",
    "Hence it becomes an optimization problem, we would like to solve for n and b such that the below objective function is maximized.\n",
    "$$max L_{n,b} = \\prod_{i=1}^k (p(x^i))^{y^i} (1 - p(x^i))^{1 - y^i}$$ \n",
    "k is the number of samples. If y=0 the first term above becomes 1 and if y=1 the second term becomes 1. That leaves us with the product of all the probabilities that should be maximized. Since optimizing products is difficult people apply log, to make it a summation and then try to optimize it.\n",
    "$$max log L_{n,b} = \\sum_{i=1}^k y^ilog p(x^i) + (1 - y^i)log (1 - p(x^i))$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42406558-6e4c-4228-8b38-860622b7cac1",
   "metadata": {},
   "source": [
    "#### Extension to nonlinear decision boundary\n",
    "h(x) changes to a non-linear equation, rest of the process remains the same\n",
    "$$h(x) = x^TQx + n^Tx + b$$\n",
    "\n",
    "#### Extension to multi class clasification\n",
    "It can be extended to multi class clasification problems by solving multiple binary classification problems. One versus rest approach, if there are 3 classes we do 3 binary clasification as above. Class 0 vs class 1 & 2, class 1 vs class 0 & 2 and class 2 vs class 0 & 1. From these clasifications we get the probalities for class 0,1 and 2 for each sample, based on which we clasify."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
